This project is a Flask-based web application that scrapes and aggregates Common Vulnerabilities and Exposures (CVE) data from multiple sources, including NIST, Vulners, and Exploit DB. The application retrieves CVE information, such as CVSS scores, descriptions, affected assets, and related exploits, and allows users to export the results in different formats (TXT, DOCX).
Dependencies

The project relies on several Python libraries and tools to function properly. Below is a list of required dependencies:

    -Flask
    -BeautifulSoup (bs4)
    -requests
    -selenium
    -webdriver_manager
    -python-docx
    -datetime

Installation Instructions

    1. Install Dependencies:
      Ensure you have Python installed on your machine. Install the required dependencies by running:
      pip install -r requirements.txt

    2. Running the Application:
      To start the application, navigate to the project directory and run the Flask server:
      python app.py

    This will start the server, and the web application can be accessed at http://127.0.0.1:5000/.

    Exporting Data:
    Use the web interface to enter CVE numbers, and the app will scrape and aggregate data from multiple sources. The results can be exported in either TXT or DOCX formats by choosing the desired file format on the web form.

Group Members and Roles

    Sarfi Habibova: Scraped data from NIST and Exploit DB, prepared the README.md file, and provided support for other parts of the code.
    Elvin Madatov: Scraped data from MITRE and developed the input validation code. Also he did AI integration to summarize data from different resources.
    Aynur Heydarova: Scraped data from Vulmon, designed, and prepared the user interface.
    Rauf Ramaldanov: Scraped data from Vulners.

Additional Comments

All team members contributed to various aspects of the code and provided support to one another throughout the project. After gathering data from each source, the team collaboratively decided on the most appropriate sources for the project. Initially, we integrated AI functionality using the OpenAI API to summarize the data output. However, after further discussion, the team decided that utilizing a few sources would be more efficient for the project. As a result, we removed the AI-based code to streamline our approach and maintain focus on data accuracy from the selected sources.
   
